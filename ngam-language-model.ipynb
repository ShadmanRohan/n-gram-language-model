{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ngam-language-model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShadmanRohan/n-gram-language-model/blob/master/ngam-language-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C3mevTLyNXU",
        "colab_type": "code",
        "outputId": "d71d678e-60fa-4c55-bb1d-8eba85f5b957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc19e033090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K50cEUzzyNXn",
        "colab_type": "text"
      },
      "source": [
        "### Embedding Layer in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v3y6DnPyNXq",
        "colab_type": "code",
        "outputId": "9408eee7-e732-4f7f-fe6d-60b494794a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "word_to_ix = {\"hello\": 0, \"world\": 1} # put two words in a dictionary\n",
        "embeds = nn.Embedding(3, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
        "print(embeds)\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"world\"]], dtype=torch.long) # torch.long is an Integer, returns the index\n",
        "print(lookup_tensor)\n",
        "hello_embed = embeds(torch.tensor([0])) # callable returns the vector in the index\n",
        "#print(torch.tensor([4]))\n",
        "print(hello_embed) # randomly generated vector"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding(3, 5)\n",
            "tensor([1])\n",
            "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
            "       grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlqIF-RkyNX8",
        "colab_type": "code",
        "outputId": "d0beecc9-7d7a-469e-f4a5-32cb1e09ca5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "embedding = nn.Embedding(1000,128) # randomly initialised matrix, 10000 words * 128 dim each\n",
        "print(embedding(torch.LongTensor([3,6]))) # vocabulary indexed at 3 and 6\n",
        "print(embedding(torch.LongTensor([6]))) # vocabulary index at 6 only\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-4.5518e-01,  3.1860e-01, -3.5494e-01,  6.8589e-01, -3.7614e-01,\n",
            "         -2.4107e+00, -1.2778e+00, -6.2887e-02, -9.4713e-02, -2.3144e+00,\n",
            "          5.5653e-01,  5.0569e-01, -2.0760e-01,  6.9363e-01,  4.1949e-01,\n",
            "          2.2524e+00,  9.3852e-01,  1.4253e+00,  1.5083e+00,  1.0539e-01,\n",
            "         -1.6050e+00, -1.0645e-01,  2.4657e-01,  6.1251e-01,  7.3980e-01,\n",
            "         -1.7860e-01,  7.8490e-02, -4.3982e-01, -3.6079e-01, -1.2617e+00,\n",
            "          1.9147e+00, -1.8613e+00, -9.6749e-03,  2.6039e-01,  2.8203e-01,\n",
            "          2.5830e-01, -4.2655e-01,  9.8075e-01,  1.8589e+00, -1.0920e+00,\n",
            "          7.6300e-01,  2.2762e-01, -1.4570e+00,  1.7044e+00, -3.2686e+00,\n",
            "          4.7499e-01, -2.1142e+00, -1.5002e+00,  1.0693e+00,  1.4394e+00,\n",
            "          5.0646e-01,  8.3598e-01,  1.1753e+00, -3.4212e-01, -3.8716e-01,\n",
            "          5.4765e-01, -1.5892e-01, -7.3605e-01, -2.3352e-01, -5.4039e-01,\n",
            "          1.5708e-01, -5.9762e-01, -8.8391e-01,  6.0767e-01, -3.8844e-01,\n",
            "         -3.1579e-02, -5.6059e-01, -6.5552e-01,  7.2615e-01,  6.7892e-01,\n",
            "         -4.3017e-01, -3.8485e-01, -1.5083e+00, -7.1996e-01, -1.1910e+00,\n",
            "          1.3271e+00, -2.1984e+00,  2.8614e-01, -2.0105e-01, -2.5349e+00,\n",
            "         -1.5848e+00,  2.1679e-01, -1.4276e-01,  1.4274e+00,  1.6425e-01,\n",
            "         -3.1607e-01,  1.2852e-01, -5.2765e-01,  1.0834e+00,  7.2746e-01,\n",
            "          5.7726e-01,  5.3688e-01, -4.3616e-01,  2.7677e-01,  2.9459e-01,\n",
            "         -5.6315e-01,  5.1899e-01,  1.3395e+00, -2.3876e-01, -6.7961e-02,\n",
            "         -1.5036e-01,  5.2330e-01, -2.1156e-01, -1.2542e+00,  1.8176e-02,\n",
            "          1.4142e+00, -1.7438e+00,  1.1289e-01,  4.5267e-01,  3.1554e-01,\n",
            "         -6.9010e-01, -2.8290e-01,  3.5618e-01, -6.5617e-01,  6.7500e-01,\n",
            "          1.2910e+00,  2.8768e-01,  1.1313e+00, -1.9228e-03, -2.3545e-01,\n",
            "         -7.7834e-01,  1.7675e-02,  1.1870e+00, -5.9569e-01, -1.5739e+00,\n",
            "          9.0095e-01,  1.0499e+00,  4.2926e-01],\n",
            "        [ 1.3378e+00, -7.1273e-02, -1.6469e+00,  8.4098e-01, -3.1005e-01,\n",
            "         -1.0481e+00, -9.3338e-01,  2.4531e-01,  3.4689e-01,  2.5516e+00,\n",
            "         -8.0720e-01,  3.3628e+00, -7.3907e-01,  2.2798e-01,  9.8850e-01,\n",
            "         -2.1413e+00, -1.8412e+00, -6.1972e-01, -1.1801e+00, -8.5096e-01,\n",
            "          1.2851e+00, -1.2276e+00, -3.7219e-01,  1.0823e+00, -7.1018e-01,\n",
            "          2.8641e+00,  1.0953e+00,  7.7987e-01,  1.7714e+00,  1.7485e+00,\n",
            "         -6.5975e-01, -1.1961e+00, -2.6117e-01, -6.1217e-04,  1.5526e+00,\n",
            "         -9.2554e-01, -8.4613e-01,  1.5879e-01,  8.5641e-01, -2.0898e+00,\n",
            "         -1.2149e+00,  1.7847e-01,  6.9572e-01,  3.1242e-02, -2.2674e+00,\n",
            "          9.3775e-02,  4.8207e-01,  8.2691e-02,  8.7677e-01,  1.4545e+00,\n",
            "          3.7237e-02,  9.6609e-01,  1.9585e-01, -9.5072e-01,  1.3012e-01,\n",
            "          8.6300e-01,  4.9444e-01,  1.1529e+00,  1.7298e-01,  3.9719e-01,\n",
            "         -4.4768e-01,  4.5878e-01,  7.3751e-01, -1.1821e+00, -8.0106e-01,\n",
            "         -1.5776e+00, -9.1714e-01, -2.3112e-01,  1.0470e+00, -1.5918e+00,\n",
            "          8.1379e-02,  1.0832e+00, -5.7936e-01, -5.9484e-01,  7.1444e-02,\n",
            "          3.4204e-01,  8.8663e-01, -8.9544e-01,  1.3602e-01,  6.5794e-01,\n",
            "         -9.1017e-01, -1.4234e-01,  2.9891e-01,  1.4571e+00,  2.3037e-01,\n",
            "         -1.4793e-01, -6.3583e-01,  3.5349e-01, -3.2089e-02, -5.6836e-01,\n",
            "         -1.4244e+00, -1.3247e+00, -2.0823e+00, -6.3228e-01,  1.5993e+00,\n",
            "          1.9342e+00,  5.9308e-01,  1.8194e+00, -8.7919e-01, -1.1781e+00,\n",
            "          2.5037e-01,  3.6793e-01, -5.0982e-01, -9.9217e-02, -5.0827e-01,\n",
            "          1.2397e+00,  4.2371e-01,  2.6685e-01,  7.6762e-02, -1.0789e+00,\n",
            "          7.9333e-01, -6.1699e-01, -1.5175e+00, -1.0135e+00,  1.3760e+00,\n",
            "          1.4397e+00, -2.2479e+00,  1.3209e+00, -5.3911e-01,  3.8984e-01,\n",
            "         -8.4675e-01, -2.1516e+00,  2.4737e-01, -1.4464e+00, -1.2147e+00,\n",
            "          2.8860e-01, -8.5133e-01, -8.6620e-01]], grad_fn=<EmbeddingBackward>)\n",
            "tensor([[ 1.3378e+00, -7.1273e-02, -1.6469e+00,  8.4098e-01, -3.1005e-01,\n",
            "         -1.0481e+00, -9.3338e-01,  2.4531e-01,  3.4689e-01,  2.5516e+00,\n",
            "         -8.0720e-01,  3.3628e+00, -7.3907e-01,  2.2798e-01,  9.8850e-01,\n",
            "         -2.1413e+00, -1.8412e+00, -6.1972e-01, -1.1801e+00, -8.5096e-01,\n",
            "          1.2851e+00, -1.2276e+00, -3.7219e-01,  1.0823e+00, -7.1018e-01,\n",
            "          2.8641e+00,  1.0953e+00,  7.7987e-01,  1.7714e+00,  1.7485e+00,\n",
            "         -6.5975e-01, -1.1961e+00, -2.6117e-01, -6.1217e-04,  1.5526e+00,\n",
            "         -9.2554e-01, -8.4613e-01,  1.5879e-01,  8.5641e-01, -2.0898e+00,\n",
            "         -1.2149e+00,  1.7847e-01,  6.9572e-01,  3.1242e-02, -2.2674e+00,\n",
            "          9.3775e-02,  4.8207e-01,  8.2691e-02,  8.7677e-01,  1.4545e+00,\n",
            "          3.7237e-02,  9.6609e-01,  1.9585e-01, -9.5072e-01,  1.3012e-01,\n",
            "          8.6300e-01,  4.9444e-01,  1.1529e+00,  1.7298e-01,  3.9719e-01,\n",
            "         -4.4768e-01,  4.5878e-01,  7.3751e-01, -1.1821e+00, -8.0106e-01,\n",
            "         -1.5776e+00, -9.1714e-01, -2.3112e-01,  1.0470e+00, -1.5918e+00,\n",
            "          8.1379e-02,  1.0832e+00, -5.7936e-01, -5.9484e-01,  7.1444e-02,\n",
            "          3.4204e-01,  8.8663e-01, -8.9544e-01,  1.3602e-01,  6.5794e-01,\n",
            "         -9.1017e-01, -1.4234e-01,  2.9891e-01,  1.4571e+00,  2.3037e-01,\n",
            "         -1.4793e-01, -6.3583e-01,  3.5349e-01, -3.2089e-02, -5.6836e-01,\n",
            "         -1.4244e+00, -1.3247e+00, -2.0823e+00, -6.3228e-01,  1.5993e+00,\n",
            "          1.9342e+00,  5.9308e-01,  1.8194e+00, -8.7919e-01, -1.1781e+00,\n",
            "          2.5037e-01,  3.6793e-01, -5.0982e-01, -9.9217e-02, -5.0827e-01,\n",
            "          1.2397e+00,  4.2371e-01,  2.6685e-01,  7.6762e-02, -1.0789e+00,\n",
            "          7.9333e-01, -6.1699e-01, -1.5175e+00, -1.0135e+00,  1.3760e+00,\n",
            "          1.4397e+00, -2.2479e+00,  1.3209e+00, -5.3911e-01,  3.8984e-01,\n",
            "         -8.4675e-01, -2.1516e+00,  2.4737e-01, -1.4464e+00, -1.2147e+00,\n",
            "          2.8860e-01, -8.5133e-01, -8.6620e-01]], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnEyO0Y0yNYI",
        "colab_type": "text"
      },
      "source": [
        "### Train a Tri-gram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWmUY3hZyNYJ",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-toT8-QCyNYK",
        "colab_type": "code",
        "outputId": "eed52d30-c2ef-481b-afd9-d099471dd39a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "CONTEXT_SIZE = 2 # parameters\n",
        "EMBEDDING_DIM = 10 # word embedding dimension\n",
        "# Some random bangla news paper article\n",
        "test_sentence = \"\"\"প্রথমেই শুরু হয়েছে স্বাস্থ্য ও চিকিৎসার সংকট। এতে সরকারকে জরুরি স্বাস্থ্য বরাদ্দ বাড়াতে হবে। অন্যথায় স্বাস্থ্য ও চিকিৎসার \n",
        "সংকট প্রলম্বিত হয়ে অনুৎপাদনশীলতার জন্ম হবে, যার আর্থিক দায় ব্যাপক। দ্বিতীয় সংকট হতে পারে খাদ্য ও মানবিক সংকট। যেহেতু সংক্রমণ বিস্তার \n",
        "রোধে প্রায় পূর্ণাঙ্গ ‘লকডাউন’ শুরু হয়েছে, তাই নিম্ন আয়ের মানুষ অর্থ ও সঞ্চয় সংকটে পড়বে। শুরুতেই সঞ্চয়হীন ভাসমান মানুষ, দিনমজুর, \n",
        "বৃদ্ধ-অনাথ-এতিম, রিকশা, ছোট কারখানা, নির্মাণশ্রমিক যাঁরা ‘দিন আনে দিন খান’, তাঁরা লকডাউনের দ্বিতীয় সপ্তাহ থেকেই আয় হীনতার কারণে \n",
        "খাদ্যের সংকটে পড়বেন। শহরের ভাসমান প্রান্তিক অর্থনৈতিক শ্রেণি সামাজিক উৎস থেকে ধার-ঋণ নিতে অক্ষম বলে তাদের জন্য খাদ্যসংকট অবধারিত। \n",
        "গ্রামে ‘সমাজের’ উপস্থিতি এবং কৃষি ও ক্ষুদ্রশিল্পভিত্তিক ‘উৎপাদনব্যবস্থা’ রয়েছে বিধায় সেখানে খাদ্যসংকট কিছুটা দেরিতে আসবে। গ্রামে ভাসমানদের \n",
        "কর্মহীনতার তৃতীয় কিংবা চতুর্থ সপ্তাহ থেকে খাদ্যসংকট শুরু হতে পারে, তার আগে পর্যন্ত তাঁরা চেয়েচিন্তে চলতে পারবেন হয়তো। পরেই আসবে \n",
        "কর্মহীন নিম্নবিত্ত, যাদের কিছুটা সঞ্চয় ছিল—এমন শ্রেণি। তার পরে আসবে বেতন বন্ধ হয়ে সঞ্চয় ফুরিয়ে যাওয়া নিম্ন–মধ্যবিত্ত কিংবা মধ্যবিত্তও। \n",
        "এই সব কটি প্রান্তিক ধারার জন্য জরুরি খাদ্য সরবরাহ করার একটা দায় আছে সরকারের। ইতিমধ্যেই পশ্চিমবঙ্গ সরকার সাত কোটি মানুষের \n",
        "ছয় মাসের জরুরি খাদ্য সরবরাহের ঘোষণা দিয়েছে। অন্যদিকে কেরালার সরকার কুড়ি হাজার কোটি রুপির করোনা প্যাকেজ ঘোষণা করেছে, \n",
        "এসেছে বিদ্যুৎ বিলে ছাড়ের ঘোষণা। বাংলাদেশেও মাথাপিছু ন্যূনতম ‘ক্যালরি ধারণ’ ভিত্তিতে ভাসমান প্রান্তিক শ্রেণি, স্থায়ী বেকার, তাৎক্ষণিকভাবে \n",
        "কাজহীন, বেতন বন্ধ হয়ে পড়া, সঞ্চয় ফুরিয়ে যাওয়া শ্রেণির জন্য খাদ্য সরবরাহের বাধ্যবাধকতা তৈরি হয়েছে। আর্থিক সংখ্যায় রূপান্তর করলে দেখা \n",
        "যায়, সরকারের জন্য তৈরি হয়েছে বড় এক আর্থিক বোঝা!\n",
        "\n",
        "\"\"\".split()\n",
        "# we should tokenize the input, but we will ignore that for now\n",
        "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
        "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
        "            for i in range(len(test_sentence) - 2)]\n",
        "\n",
        "# print the first 3, just so you can see what they look like\n",
        "print(trigrams[:5])\n",
        "\n",
        "vocab = set(test_sentence) # store the distinct words\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "#print(torch.LongTensor([1,2,3]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['প্রথমেই', 'শুরু'], 'হয়েছে'), (['শুরু', 'হয়েছে'], 'স্বাস্থ্য'), (['হয়েছে', 'স্বাস্থ্য'], 'ও'), (['স্বাস্থ্য', 'ও'], 'চিকিৎসার'), (['ও', 'চিকিৎসার'], 'সংকট।')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyxgkiqXyNYS",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Create and Train Model [Embedding Froozen]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezFbbEOSyNYU",
        "colab_type": "code",
        "outputId": "177c0b6d-9da3-4853-cb87-722f6df27a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1)) # view flattens the input\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "print(\"Before Training : {}\".format(model.embeddings(torch.LongTensor([1,2])))) # first few random word embeddings\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "model.embeddings.weight.requires_grad = False\n",
        "for epoch in range(200):\n",
        "    total_loss = 0\n",
        "    for context, target in trigrams:\n",
        "\n",
        "        # Prepare the inputs to be passed to the model\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long) #turn the words into integer indices and wrap them in tensors\n",
        "\n",
        "        # Accumulated Gradient must be zeroed out before computing gradient again\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass(probably using __Callable__), returns log probabilities over next words\n",
        "        log_probs = model(context_idxs)\n",
        "\n",
        "        # Compute loss function\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))  #target word wrapped in a tensor\n",
        "\n",
        "        # Let the gradient flow backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the gradient\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "    \n",
        "#print(losses)  # The loss decreased every iteration over the training data!\n",
        "\n",
        "print(\"After Training : {}\".format(model.embeddings(torch.LongTensor([1,2])))) # first few random word embeddings again"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before Training : tensor([[-1.9708, -2.2711, -2.3553, -0.0766,  0.2062, -0.1147,  0.2659, -0.5558,\n",
            "          0.2354, -0.7534],\n",
            "        [ 0.2136, -0.8176, -0.2218,  1.5490, -2.0157,  1.1105, -0.2885,  1.0748,\n",
            "         -1.5732,  0.6442]], grad_fn=<EmbeddingBackward>)\n",
            "After Training : tensor([[-1.9708, -2.2711, -2.3553, -0.0766,  0.2062, -0.1147,  0.2659, -0.5558,\n",
            "          0.2354, -0.7534],\n",
            "        [ 0.2136, -0.8176, -0.2218,  1.5490, -2.0157,  1.1105, -0.2885,  1.0748,\n",
            "         -1.5732,  0.6442]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZbi0hLORi8i",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Create and Train Model [Embedding Trainable]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaG9FhsbyNYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "361a048a-d21a-4c65-990b-a02af4ad701c"
      },
      "source": [
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1)) # view flattens the input\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "print(\"Before Training : {}\".format(model.embeddings(torch.LongTensor([1,2])))) # first few random word embeddings\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "# model.embeddings.weight.requires_grad = False\n",
        "for epoch in range(200):\n",
        "    total_loss = 0\n",
        "    for context, target in trigrams:\n",
        "\n",
        "        # Prepare the inputs to be passed to the model\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long) #turn the words into integer indices and wrap them in tensors\n",
        "\n",
        "        # Accumulated Gradient must be zeroed out before computing gradient again\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass(probably using __Callable__), returns log probabilities over next words\n",
        "        log_probs = model(context_idxs)\n",
        "\n",
        "        # Compute loss function\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))  #target word wrapped in a tensor\n",
        "\n",
        "        # Let the gradient flow backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the gradient\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "    \n",
        "#print(losses)  # The loss decreased every iteration over the training data!\n",
        "\n",
        "print(\"After Training : {}\".format(model.embeddings(torch.LongTensor([1,2])))) # first few random word embeddings again"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before Training : tensor([[ 0.9915,  0.6616, -0.3415, -0.8382, -1.0565,  0.9416,  0.6389, -0.3896,\n",
            "         -0.1721, -2.0031],\n",
            "        [ 3.1797, -1.1324, -0.9324,  1.3209,  0.8647, -0.5295, -0.4210, -1.0535,\n",
            "          0.2932, -0.2520]], grad_fn=<EmbeddingBackward>)\n",
            "After Training : tensor([[ 1.0526,  0.6743, -0.3418, -0.8707, -1.0880,  0.9743,  0.6368, -0.3906,\n",
            "         -0.1742, -2.0610],\n",
            "        [ 3.2230, -1.1339, -0.9538,  1.3542,  0.9175, -0.5268, -0.4367, -1.0652,\n",
            "          0.3058, -0.2182]], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
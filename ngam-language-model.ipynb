{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ngam-language-model.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShadmanRohan/n-gram-language-model/blob/master/ngam-language-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C3mevTLyNXU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "46e6db70-99cb-4d6f-bf61-60f6d74bed33"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7faccede4e50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K50cEUzzyNXn",
        "colab_type": "text"
      },
      "source": [
        "### Embedding Layer in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v3y6DnPyNXq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "929c4497-7615-4c4f-9dc1-e421a83763ef"
      },
      "source": [
        "word_to_ix = {\"hello\": 0, \"world\": 1} # put two words in a dictionary\n",
        "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"world\"]], dtype=torch.long) # torch.long is an Integer, returns the index\n",
        "print(lookup_tensor)\n",
        "hello_embed = embeds(lookup_tensor) # callable returns the vector in the index\n",
        "print(hello_embed) # randomly generated vector"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1])\n",
            "tensor([[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]],\n",
            "       grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlqIF-RkyNX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "732037f8-5e85-4410-9c41-21213f262aa4"
      },
      "source": [
        "embedding = nn.Embedding(1000,128) # randomly initialised matrix, 10000 words * 128 dim each\n",
        "embedding(torch.LongTensor([3,6])) # return 128 dim vector"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.8895e-01,  1.5219e-01, -2.9803e-01, -1.3135e-01,  1.5365e+00,\n",
              "          9.1930e-01,  3.4535e-01, -5.5059e-01, -9.6037e-01,  2.7690e-01,\n",
              "         -6.8011e-01, -5.4579e-01,  1.1528e+00, -3.9975e-01,  9.7380e-01,\n",
              "          1.5453e+00, -3.7614e-01, -2.4107e+00, -1.2778e+00, -6.2887e-02,\n",
              "         -1.2308e+00, -1.0655e+00, -3.8646e-01,  3.7823e-01, -2.0760e-01,\n",
              "          6.9363e-01,  4.1949e-01,  2.2524e+00, -8.0162e-02, -1.3489e+00,\n",
              "         -8.2630e-01,  2.6483e-01, -1.6050e+00, -1.0645e-01,  2.4657e-01,\n",
              "          6.1251e-01, -7.6937e-03,  2.6158e+00, -6.3858e-02,  2.6087e-01,\n",
              "         -3.6079e-01, -1.2617e+00,  1.9147e+00, -1.8613e+00,  4.7390e-01,\n",
              "          9.9882e-01, -1.1198e-01, -1.7404e+00, -4.2655e-01,  9.8075e-01,\n",
              "          1.8589e+00, -1.0920e+00, -7.1291e-01, -6.3866e-02,  1.0757e+00,\n",
              "         -5.5361e-01, -3.2686e+00,  4.7499e-01, -2.1142e+00, -1.5002e+00,\n",
              "          2.6929e-01, -4.8861e-01,  1.3694e+00,  4.5392e-01,  1.1753e+00,\n",
              "         -3.4212e-01, -3.8716e-01,  5.4765e-01, -8.0262e-01,  3.0364e-01,\n",
              "         -2.0311e+00, -1.1064e+00,  1.5708e-01, -5.9762e-01, -8.8391e-01,\n",
              "          6.0767e-01, -2.6189e+00,  2.2544e+00, -1.1085e+00, -1.8874e+00,\n",
              "          7.2615e-01,  6.7892e-01, -4.3017e-01, -3.8485e-01,  8.5646e-01,\n",
              "         -3.2585e-03,  7.9233e-01, -3.3846e-01, -2.1984e+00,  2.8614e-01,\n",
              "         -2.0105e-01, -2.5349e+00, -1.3196e+00,  1.6485e+00,  1.2849e+00,\n",
              "         -8.1589e-01,  1.6425e-01, -3.1607e-01,  1.2852e-01, -5.2765e-01,\n",
              "          7.1484e-01,  3.9693e-01,  1.9269e-01,  3.4289e-01, -4.3616e-01,\n",
              "          2.7677e-01,  2.9459e-01, -5.6315e-01,  7.2521e-01, -5.4636e-01,\n",
              "         -8.0242e-01,  1.8642e-02, -1.5036e-01,  5.2330e-01, -2.1156e-01,\n",
              "         -1.2542e+00, -3.6890e-02,  1.8197e-01, -1.2673e+00, -5.9425e-01,\n",
              "          4.5267e-01,  3.1554e-01, -6.9010e-01, -2.8290e-01,  9.9063e-02,\n",
              "          4.9380e-01, -1.0002e+00, -6.8304e-01],\n",
              "        [ 1.3420e+00, -6.6578e-01, -2.1530e-01,  3.3213e-01, -9.3401e-01,\n",
              "          1.6575e+00,  1.0078e+00,  1.3923e+00, -1.7940e+00,  1.3682e+00,\n",
              "          2.2178e-02,  9.8350e-01, -8.2537e-01, -4.0866e-01, -2.3702e+00,\n",
              "         -2.1516e-02, -3.1005e-01, -1.0481e+00, -9.3338e-01,  2.4531e-01,\n",
              "          1.7929e-01,  7.6350e-01, -1.9245e-01, -6.8592e-01, -7.3907e-01,\n",
              "          2.2798e-01,  9.8850e-01, -2.1413e+00, -2.2426e-01, -6.5671e-02,\n",
              "         -1.2895e+00,  5.9075e-02,  1.2851e+00, -1.2276e+00, -3.7219e-01,\n",
              "          1.0823e+00, -1.2426e+00,  8.1323e-01,  9.0975e-02, -5.7014e-01,\n",
              "          1.7714e+00,  1.7485e+00, -6.5975e-01, -1.1961e+00, -3.0230e-01,\n",
              "          8.0996e-02, -9.7508e-01,  7.9828e-01, -8.4613e-01,  1.5879e-01,\n",
              "          8.5641e-01, -2.0898e+00, -1.2691e+00,  3.2928e-01,  3.7079e-01,\n",
              "         -1.0255e+00, -2.2674e+00,  9.3775e-02,  4.8207e-01,  8.2691e-02,\n",
              "          9.5343e-01, -6.8439e-01,  3.6302e-02,  5.5979e-01,  1.9585e-01,\n",
              "         -9.5072e-01,  1.3012e-01,  8.6300e-01, -3.1241e-01,  2.5639e-02,\n",
              "         -3.9241e-01,  3.1191e-01, -4.4768e-01,  4.5878e-01,  7.3751e-01,\n",
              "         -1.1821e+00,  2.7002e-01, -4.7428e-01,  5.7737e-01,  1.7577e-01,\n",
              "          1.0470e+00, -1.5918e+00,  8.1379e-02,  1.0832e+00, -7.2124e-01,\n",
              "          6.9948e-01, -6.6051e-01,  5.6863e-01,  8.8663e-01, -8.9544e-01,\n",
              "          1.3602e-01,  6.5794e-01,  1.1448e+00,  1.0681e+00, -9.4928e-01,\n",
              "         -7.2223e-01,  2.3037e-01, -1.4793e-01, -6.3583e-01,  3.5349e-01,\n",
              "         -9.6157e-02,  1.3376e+00, -1.2658e+00, -2.0567e+00, -2.0823e+00,\n",
              "         -6.3228e-01,  1.5993e+00,  1.9342e+00,  1.1831e+00, -8.3254e-01,\n",
              "          1.2053e+00, -2.3208e-01,  2.5037e-01,  3.6793e-01, -5.0982e-01,\n",
              "         -9.9217e-02,  3.7935e-01,  4.1680e-01, -7.5422e-01,  7.9217e-01,\n",
              "          7.6762e-02, -1.0789e+00,  7.9333e-01, -6.1699e-01, -2.0545e+00,\n",
              "         -1.6232e-01, -7.4177e-01, -7.8852e-01]], grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnEyO0Y0yNYI",
        "colab_type": "text"
      },
      "source": [
        "### Train a Tri-gram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWmUY3hZyNYJ",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-toT8-QCyNYK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e7ab7621-c01a-4d75-9243-33fb0e9973dc"
      },
      "source": [
        "CONTEXT_SIZE = 2 # parameters\n",
        "EMBEDDING_DIM = 10 # word embedding dimension\n",
        "# Some random bangla news paper article\n",
        "test_sentence = \"\"\"প্রথমেই শুরু হয়েছে স্বাস্থ্য ও চিকিৎসার সংকট। এতে সরকারকে জরুরি স্বাস্থ্য বরাদ্দ বাড়াতে হবে। অন্যথায় স্বাস্থ্য ও চিকিৎসার \n",
        "সংকট প্রলম্বিত হয়ে অনুৎপাদনশীলতার জন্ম হবে, যার আর্থিক দায় ব্যাপক। দ্বিতীয় সংকট হতে পারে খাদ্য ও মানবিক সংকট। যেহেতু সংক্রমণ বিস্তার \n",
        "রোধে প্রায় পূর্ণাঙ্গ ‘লকডাউন’ শুরু হয়েছে, তাই নিম্ন আয়ের মানুষ অর্থ ও সঞ্চয় সংকটে পড়বে। শুরুতেই সঞ্চয়হীন ভাসমান মানুষ, দিনমজুর, \n",
        "বৃদ্ধ-অনাথ-এতিম, রিকশা, ছোট কারখানা, নির্মাণশ্রমিক যাঁরা ‘দিন আনে দিন খান’, তাঁরা লকডাউনের দ্বিতীয় সপ্তাহ থেকেই আয় হীনতার কারণে \n",
        "খাদ্যের সংকটে পড়বেন। শহরের ভাসমান প্রান্তিক অর্থনৈতিক শ্রেণি সামাজিক উৎস থেকে ধার-ঋণ নিতে অক্ষম বলে তাদের জন্য খাদ্যসংকট অবধারিত। \n",
        "গ্রামে ‘সমাজের’ উপস্থিতি এবং কৃষি ও ক্ষুদ্রশিল্পভিত্তিক ‘উৎপাদনব্যবস্থা’ রয়েছে বিধায় সেখানে খাদ্যসংকট কিছুটা দেরিতে আসবে। গ্রামে ভাসমানদের \n",
        "কর্মহীনতার তৃতীয় কিংবা চতুর্থ সপ্তাহ থেকে খাদ্যসংকট শুরু হতে পারে, তার আগে পর্যন্ত তাঁরা চেয়েচিন্তে চলতে পারবেন হয়তো। পরেই আসবে \n",
        "কর্মহীন নিম্নবিত্ত, যাদের কিছুটা সঞ্চয় ছিল—এমন শ্রেণি। তার পরে আসবে বেতন বন্ধ হয়ে সঞ্চয় ফুরিয়ে যাওয়া নিম্ন–মধ্যবিত্ত কিংবা মধ্যবিত্তও। \n",
        "এই সব কটি প্রান্তিক ধারার জন্য জরুরি খাদ্য সরবরাহ করার একটা দায় আছে সরকারের। ইতিমধ্যেই পশ্চিমবঙ্গ সরকার সাত কোটি মানুষের \n",
        "ছয় মাসের জরুরি খাদ্য সরবরাহের ঘোষণা দিয়েছে। অন্যদিকে কেরালার সরকার কুড়ি হাজার কোটি রুপির করোনা প্যাকেজ ঘোষণা করেছে, \n",
        "এসেছে বিদ্যুৎ বিলে ছাড়ের ঘোষণা। বাংলাদেশেও মাথাপিছু ন্যূনতম ‘ক্যালরি ধারণ’ ভিত্তিতে ভাসমান প্রান্তিক শ্রেণি, স্থায়ী বেকার, তাৎক্ষণিকভাবে \n",
        "কাজহীন, বেতন বন্ধ হয়ে পড়া, সঞ্চয় ফুরিয়ে যাওয়া শ্রেণির জন্য খাদ্য সরবরাহের বাধ্যবাধকতা তৈরি হয়েছে। আর্থিক সংখ্যায় রূপান্তর করলে দেখা \n",
        "যায়, সরকারের জন্য তৈরি হয়েছে বড় এক আর্থিক বোঝা!\n",
        "\n",
        "\"\"\".split()\n",
        "# we should tokenize the input, but we will ignore that for now\n",
        "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
        "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
        "            for i in range(len(test_sentence) - 2)]\n",
        "\n",
        "# print the first 3, just so you can see what they look like\n",
        "print(trigrams[:3])\n",
        "\n",
        "vocab = set(test_sentence) # store the distinct words\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "print(torch.LongTensor([1,2,3]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['প্রথমেই', 'শুরু'], 'হয়েছে'), (['শুরু', 'হয়েছে'], 'স্বাস্থ্য'), (['হয়েছে', 'স্বাস্থ্য'], 'ও')]\n",
            "tensor([1, 2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyxgkiqXyNYS",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Create and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezFbbEOSyNYU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "a9e67fe4-d3f8-4ebe-ca74-8736efef28c7"
      },
      "source": [
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "print(\"Before Training : {}\".format(model.embeddings(torch.LongTensor([1])))) # first few random word embeddings\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(200):\n",
        "    total_loss = 0\n",
        "    for context, target in trigrams:\n",
        "\n",
        "        # Prepare the inputs to be passed to the model\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long) #turn the words into integer indices and wrap them in tensors\n",
        "\n",
        "        # Accumulated Gradient must be zeroed out before computing gradient again\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass(probably using __Callable__), returns log probabilities over next words\n",
        "        log_probs = model(context_idxs)\n",
        "\n",
        "        # Compute loss function\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))  #target word wrapped in a tensor\n",
        "\n",
        "        # Let the gradient flow backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the gradient\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "    \n",
        "print(losses)  # The loss decreased every iteration over the training data!\n",
        "\n",
        "print(\"After Training : {}\".format(model.embeddings(torch.LongTensor([1])))) # first few random word embeddings again"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before Training : tensor([[-1.5454, -0.2652,  0.0883, -0.6848,  1.9326,  0.6381, -0.0039, -0.1837,\n",
            "         -0.3661,  0.0090]], grad_fn=<EmbeddingBackward>)\n",
            "[1226.2994923591614, 1220.7042546272278, 1215.1553497314453, 1209.647138118744, 1204.1756258010864, 1198.7373700141907, 1193.3269543647766, 1187.937581062317, 1182.5674076080322, 1177.2122716903687, 1171.8662571907043, 1166.5256400108337, 1161.1879529953003, 1155.8527135849, 1150.5159466266632, 1145.174000263214, 1139.8196818828583, 1134.4527990818024, 1129.0716054439545, 1123.6770544052124, 1118.2629771232605, 1112.8236479759216, 1107.360211610794, 1101.8709995746613, 1096.3513269424438, 1090.802624464035, 1085.2220740318298, 1079.6060998439789, 1073.9528334140778, 1068.261400461197, 1062.5285024642944, 1056.7520174980164, 1050.9319562911987, 1045.0672054290771, 1039.1580142974854, 1033.2015960216522, 1027.1994044780731, 1021.1505630016327, 1015.0489404201508, 1008.8993911743164, 1002.698742389679, 996.4467918872833, 990.1492869853973, 983.7970972061157, 977.396909236908, 970.9426279067993, 964.438251376152, 957.8904963731766, 951.2956583499908, 944.6564193964005, 937.9680964946747, 931.2371400594711, 924.4528799057007, 917.6225868463516, 910.7471982240677, 903.8256014585495, 896.860142827034, 889.8523664474487, 882.8050825595856, 875.7150951623917, 868.5845054388046, 861.4135270118713, 854.1989387273788, 846.9397104978561, 839.6397749781609, 832.298549413681, 824.9199540019035, 817.5057170391083, 810.0562451481819, 802.5688534379005, 795.047135412693, 787.4891420006752, 779.900561273098, 772.2770312428474, 764.6252202391624, 756.9388584494591, 749.2237218022346, 741.478390276432, 733.7070511579514, 725.9057333469391, 718.0766005814075, 710.224749237299, 702.3458558022976, 694.4484483897686, 686.5316411554813, 678.5981318354607, 670.6499386429787, 662.687566101551, 654.7120436131954, 646.7294430434704, 638.7385073304176, 630.7389829158783, 622.7345604002476, 614.7271321713924, 606.7226841151714, 598.7220807671547, 590.7288579940796, 582.7476378083229, 574.7769412994385, 566.8185743093491, 558.8808861970901, 550.9599215388298, 543.0606666505337, 535.1852386891842, 527.3384970575571, 519.5187962651253, 511.73533307015896, 503.9860082268715, 496.2754603624344, 488.6038587987423, 480.97738133370876, 473.39456164836884, 465.8591900318861, 458.37462432682514, 450.94486279785633, 443.57009744644165, 436.25448974967003, 428.99468745291233, 421.80089071393013, 414.6676237285137, 407.6048552393913, 400.6071093827486, 393.6797880381346, 386.8233372569084, 380.04176335036755, 373.33252945542336, 366.7008910179138, 360.1441567838192, 353.66680513322353, 347.26807421445847, 340.9527908414602, 334.71588867902756, 328.56525053083897, 322.4967468082905, 316.51365526020527, 310.61561296880245, 304.80070082843304, 299.07630532979965, 293.43642278015614, 287.886149533093, 282.42270588874817, 277.04792200773954, 271.7609245106578, 266.56293085217476, 261.45265766978264, 256.4306329265237, 251.50069923698902, 246.65439519286156, 241.89560516923666, 237.22693019360304, 232.64207622408867, 228.1448094472289, 223.73365408927202, 219.40746727585793, 215.164932847023, 211.00859506428242, 206.9338304400444, 202.9425185099244, 199.0316499620676, 195.2022005841136, 191.45339646190405, 187.7831012904644, 184.19250569492579, 180.6760109141469, 177.23743433505297, 173.87474992871284, 170.58431904762983, 167.3681090325117, 164.22405210137367, 161.1511504650116, 158.14702481776476, 155.21133375912905, 152.3444176837802, 149.54197285324335, 146.80517330765724, 144.1332015246153, 141.52461929619312, 138.97594621777534, 136.4882801398635, 134.0612146705389, 131.69001524150372, 129.37660099565983, 127.11829649657011, 124.91456757485867, 122.76433508843184, 120.66521251946688, 118.61797789484262, 116.61941156536341, 114.67078945785761, 112.76960227265954, 110.91320631280541, 109.104123018682, 107.33796460554004, 105.61536687985063, 103.93446780368686, 102.2950912937522, 100.69466477259994, 99.13338556140661, 97.60949393361807, 96.12242259085178]\n",
            "After Training : tensor([[-1.6285, -0.3126,  0.1032, -0.7477,  2.0528,  0.6702,  0.0210, -0.2099,\n",
            "         -0.4026,  0.0544]], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaG9FhsbyNYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
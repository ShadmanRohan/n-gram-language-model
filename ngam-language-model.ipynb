{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2281dde58d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1} # put two words in a dictionary\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"world\"]], dtype=torch.long) # torch.long is an Integer, returns the index\n",
    "print(lookup_tensor)\n",
    "hello_embed = embeds(lookup_tensor) # callable returns the vector in the index\n",
    "print(hello_embed) # randomly generated vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2890,  0.1522, -0.2980, -0.1314,  1.5365,  0.9193,  0.3453, -0.5506,\n",
       "         -0.9604,  0.2769, -0.6801, -0.5458,  1.1528, -0.3997,  0.9738,  1.5453,\n",
       "         -0.3761, -2.4107, -1.2778, -0.0629, -1.2308, -1.0655, -0.3865,  0.3782,\n",
       "         -0.2076,  0.6936,  0.4195,  2.2524, -0.0802, -1.3489, -0.8263,  0.2648,\n",
       "         -1.6050, -0.1064,  0.2466,  0.6125, -0.0077,  2.6158, -0.0639,  0.2609,\n",
       "         -0.3608, -1.2617,  1.9147, -1.8613,  0.4739,  0.9988, -0.1120, -1.7404,\n",
       "         -0.4265,  0.9808,  1.8589, -1.0920, -0.7129, -0.0639,  1.0757, -0.5536,\n",
       "         -3.2686,  0.4750, -2.1142, -1.5002,  0.2693, -0.4886,  1.3694,  0.4539,\n",
       "          1.1753, -0.3421, -0.3872,  0.5477, -0.8026,  0.3036, -2.0311, -1.1064,\n",
       "          0.1571, -0.5976, -0.8839,  0.6077, -2.6189,  2.2544, -1.1085, -1.8874,\n",
       "          0.7262,  0.6789, -0.4302, -0.3849,  0.8565, -0.0033,  0.7923, -0.3385,\n",
       "         -2.1984,  0.2861, -0.2010, -2.5349, -1.3196,  1.6485,  1.2849, -0.8159,\n",
       "          0.1643, -0.3161,  0.1285, -0.5277,  0.7148,  0.3969,  0.1927,  0.3429,\n",
       "         -0.4362,  0.2768,  0.2946, -0.5631,  0.7252, -0.5464, -0.8024,  0.0186,\n",
       "         -0.1504,  0.5233, -0.2116, -1.2542, -0.0369,  0.1820, -1.2673, -0.5943,\n",
       "          0.4527,  0.3155, -0.6901, -0.2829,  0.0991,  0.4938, -1.0002, -0.6830],\n",
       "        [ 1.3420, -0.6658, -0.2153,  0.3321, -0.9340,  1.6575,  1.0078,  1.3923,\n",
       "         -1.7940,  1.3682,  0.0222,  0.9835, -0.8254, -0.4087, -2.3702, -0.0215,\n",
       "         -0.3101, -1.0481, -0.9334,  0.2453,  0.1793,  0.7635, -0.1924, -0.6859,\n",
       "         -0.7391,  0.2280,  0.9885, -2.1413, -0.2243, -0.0657, -1.2895,  0.0591,\n",
       "          1.2851, -1.2276, -0.3722,  1.0823, -1.2426,  0.8132,  0.0910, -0.5701,\n",
       "          1.7714,  1.7485, -0.6598, -1.1961, -0.3023,  0.0810, -0.9751,  0.7983,\n",
       "         -0.8461,  0.1588,  0.8564, -2.0898, -1.2691,  0.3293,  0.3708, -1.0255,\n",
       "         -2.2674,  0.0938,  0.4821,  0.0827,  0.9534, -0.6844,  0.0363,  0.5598,\n",
       "          0.1958, -0.9507,  0.1301,  0.8630, -0.3124,  0.0256, -0.3924,  0.3119,\n",
       "         -0.4477,  0.4588,  0.7375, -1.1821,  0.2700, -0.4743,  0.5774,  0.1758,\n",
       "          1.0470, -1.5918,  0.0814,  1.0832, -0.7212,  0.6995, -0.6605,  0.5686,\n",
       "          0.8866, -0.8954,  0.1360,  0.6579,  1.1448,  1.0681, -0.9493, -0.7222,\n",
       "          0.2304, -0.1479, -0.6358,  0.3535, -0.0962,  1.3376, -1.2658, -2.0567,\n",
       "         -2.0823, -0.6323,  1.5993,  1.9342,  1.1831, -0.8325,  1.2053, -0.2321,\n",
       "          0.2504,  0.3679, -0.5098, -0.0992,  0.3793,  0.4168, -0.7542,  0.7922,\n",
       "          0.0768, -1.0789,  0.7933, -0.6170, -2.0545, -0.1623, -0.7418, -0.7885]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(1000,128) # randomly initialised matrix, 10000 words * 128 dim each\n",
    "embedding(torch.LongTensor([3,6])) # return 128 dim vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Tri-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['প্রথমেই', 'শুরু'], 'হয়েছে'), (['শুরু', 'হয়েছে'], 'স্বাস্থ্য'), (['হয়েছে', 'স্বাস্থ্য'], 'ও')]\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2 # parameters\n",
    "EMBEDDING_DIM = 10 # word embedding dimension\n",
    "# Some random bangla news paper article\n",
    "test_sentence = \"\"\"প্রথমেই শুরু হয়েছে স্বাস্থ্য ও চিকিৎসার সংকট। এতে সরকারকে জরুরি স্বাস্থ্য বরাদ্দ বাড়াতে হবে। অন্যথায় স্বাস্থ্য ও চিকিৎসার \n",
    "সংকট প্রলম্বিত হয়ে অনুৎপাদনশীলতার জন্ম হবে, যার আর্থিক দায় ব্যাপক। দ্বিতীয় সংকট হতে পারে খাদ্য ও মানবিক সংকট। যেহেতু সংক্রমণ বিস্তার \n",
    "রোধে প্রায় পূর্ণাঙ্গ ‘লকডাউন’ শুরু হয়েছে, তাই নিম্ন আয়ের মানুষ অর্থ ও সঞ্চয় সংকটে পড়বে। শুরুতেই সঞ্চয়হীন ভাসমান মানুষ, দিনমজুর, \n",
    "বৃদ্ধ-অনাথ-এতিম, রিকশা, ছোট কারখানা, নির্মাণশ্রমিক যাঁরা ‘দিন আনে দিন খান’, তাঁরা লকডাউনের দ্বিতীয় সপ্তাহ থেকেই আয় হীনতার কারণে \n",
    "খাদ্যের সংকটে পড়বেন। শহরের ভাসমান প্রান্তিক অর্থনৈতিক শ্রেণি সামাজিক উৎস থেকে ধার-ঋণ নিতে অক্ষম বলে তাদের জন্য খাদ্যসংকট অবধারিত। \n",
    "গ্রামে ‘সমাজের’ উপস্থিতি এবং কৃষি ও ক্ষুদ্রশিল্পভিত্তিক ‘উৎপাদনব্যবস্থা’ রয়েছে বিধায় সেখানে খাদ্যসংকট কিছুটা দেরিতে আসবে। গ্রামে ভাসমানদের \n",
    "কর্মহীনতার তৃতীয় কিংবা চতুর্থ সপ্তাহ থেকে খাদ্যসংকট শুরু হতে পারে, তার আগে পর্যন্ত তাঁরা চেয়েচিন্তে চলতে পারবেন হয়তো। পরেই আসবে \n",
    "কর্মহীন নিম্নবিত্ত, যাদের কিছুটা সঞ্চয় ছিল—এমন শ্রেণি। তার পরে আসবে বেতন বন্ধ হয়ে সঞ্চয় ফুরিয়ে যাওয়া নিম্ন–মধ্যবিত্ত কিংবা মধ্যবিত্তও। \n",
    "এই সব কটি প্রান্তিক ধারার জন্য জরুরি খাদ্য সরবরাহ করার একটা দায় আছে সরকারের। ইতিমধ্যেই পশ্চিমবঙ্গ সরকার সাত কোটি মানুষের \n",
    "ছয় মাসের জরুরি খাদ্য সরবরাহের ঘোষণা দিয়েছে। অন্যদিকে কেরালার সরকার কুড়ি হাজার কোটি রুপির করোনা প্যাকেজ ঘোষণা করেছে, \n",
    "এসেছে বিদ্যুৎ বিলে ছাড়ের ঘোষণা। বাংলাদেশেও মাথাপিছু ন্যূনতম ‘ক্যালরি ধারণ’ ভিত্তিতে ভাসমান প্রান্তিক শ্রেণি, স্থায়ী বেকার, তাৎক্ষণিকভাবে \n",
    "কাজহীন, বেতন বন্ধ হয়ে পড়া, সঞ্চয় ফুরিয়ে যাওয়া শ্রেণির জন্য খাদ্য সরবরাহের বাধ্যবাধকতা তৈরি হয়েছে। আর্থিক সংখ্যায় রূপান্তর করলে দেখা \n",
    "যায়, সরকারের জন্য তৈরি হয়েছে বড় এক আর্থিক বোঝা!\n",
    "\n",
    "\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence) # store the distinct words\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "print(torch.LongTensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training : tensor([[ 2.1368,  0.3999, -0.9611,  0.5595,  0.0067, -1.2820, -0.2506,  0.7210,\n",
      "         -1.4951, -2.1657]], grad_fn=<EmbeddingBackward>)\n",
      "[1224.182026386261, 1219.532974243164, 1214.9117531776428, 1210.3184151649475, 1205.751760482788, 1201.205222606659, 1196.674222946167, 1192.1564030647278, 1187.6509447097778, 1183.153483390808, 1178.6650738716125, 1174.180338859558, 1169.69566488266, 1165.2082839012146, 1160.718614578247, 1156.2194356918335, 1151.7148303985596, 1147.197231054306, 1142.6698923110962, 1138.1287722587585, 1133.5722846984863, 1128.9994490146637, 1124.405119419098, 1119.784811258316, 1115.1405353546143, 1110.468938112259, 1105.7712438106537, 1101.042247056961, 1096.2829625606537, 1091.488950252533, 1086.6645658016205, 1081.8031723499298, 1076.9059875011444, 1071.9665348529816, 1066.9859828948975, 1061.9641253948212, 1056.9028885364532, 1051.7991847991943, 1046.6502017974854, 1041.4545822143555, 1036.2167675495148, 1030.9300365447998, 1025.5955843925476, 1020.2106847763062, 1014.7772071361542, 1009.2903923988342, 1003.7550382614136, 998.1707389354706, 992.5335667133331, 986.8480961322784, 981.1121697425842, 975.3205182552338, 969.4782757759094, 963.5814168453217, 957.6323883533478, 951.6297883987427, 945.5709004402161, 939.4551525115967, 933.2845344543457, 927.0569400787354, 920.776718378067, 914.4385676383972, 908.0456681251526, 901.6014368534088, 895.1066970825195, 888.5616221427917, 881.9615516662598, 875.3074636459351, 868.6006124019623, 861.8432886600494, 855.0350062847137, 848.177806854248, 841.267870426178, 834.3069925308228, 827.2978944778442, 820.2393705844879, 813.1331150531769, 805.9834313392639, 798.7857241630554, 791.5443551540375, 784.2588596343994, 776.9321074485779, 769.5680956840515, 762.1626517772675, 754.7186951637268, 747.235897064209, 739.7166447639465, 732.162752866745, 724.5749001502991, 716.9571945667267, 709.3131551742554, 701.6423270702362, 693.9455046653748, 686.2264521121979, 678.4921584129333, 670.7325656414032, 662.9581408500671, 655.166791677475, 647.3639769554138, 639.5524365901947, 631.7290096282959, 623.9021265506744, 616.0704352855682, 608.2373046875, 600.4032850265503, 592.5792062282562, 584.756668806076, 576.9451763629913, 569.1463837623596, 561.3599305152893, 553.5905520915985, 545.8389956951141, 538.1083846092224, 530.4027307033539, 522.7210378646851, 515.0677626132965, 507.447062253952, 499.8592953681946, 492.3098473548889, 484.79931640625, 477.3282632827759, 469.90179419517517, 462.5218117237091, 455.19109058380127, 447.9132080078125, 440.69006419181824, 433.5234205722809, 426.4139652252197, 419.36939883232117, 412.3871748447418, 405.4687831401825, 398.6203465461731, 391.83808994293213, 385.12966561317444, 378.4930970668793, 371.93149971961975, 365.4497537612915, 359.0434377193451, 352.71478843688965, 346.46972608566284, 340.30137276649475, 334.21975803375244, 328.2211318016052, 322.306006193161, 316.47639751434326, 310.73510098457336, 305.07785964012146, 299.5111770629883, 294.0309557914734, 288.638236284256, 283.33596301078796, 278.11961698532104, 272.9926278591156, 267.95196056365967, 262.99964594841003, 258.1365427970886, 253.35910820960999, 248.669331073761, 244.0659921169281, 239.5487940311432, 235.11772465705872, 230.77100467681885, 226.50946760177612, 222.32989263534546, 218.2345004081726, 214.22100591659546, 210.28908681869507, 206.43609714508057, 202.6634442806244, 198.96708846092224, 195.3483510017395, 191.80486750602722, 188.3372368812561, 184.9424593448639, 181.62088561058044, 178.37100481987, 175.1900975704193, 172.07810187339783, 169.0361626148224, 166.05954718589783, 163.1472601890564, 160.30213141441345, 157.5182991027832, 154.795428276062, 152.13485503196716, 149.5321388244629, 146.98737812042236, 144.4999222755432, 142.06748414039612, 139.69017887115479, 137.3649935722351, 135.09335470199585, 132.8708779811859, 130.70174717903137, 128.57820463180542, 126.50356578826904, 124.47506308555603, 122.49270629882812, 120.55397772789001, 118.66013383865356]\n",
      "After Training : tensor([[ 2.1994,  0.4245, -1.0064,  0.5699,  0.0090, -1.2826, -0.2725,  0.7308,\n",
      "         -1.5346, -2.2428]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "print(\"Before Training : {}\".format(model.embeddings(torch.LongTensor([1])))) # first few random word embeddings\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(200):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Prepare the inputs to be passed to the model\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long) #turn the words into integer indices and wrap them in tensors\n",
    "\n",
    "        # Accumulated Gradient must be zeroed out before computing gradient again\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass(probably using __Callable__), returns log probabilities over next words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Compute loss function\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))  #target word wrapped in a tensor\n",
    "\n",
    "        # Let the gradient flow backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    \n",
    "print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "print(\"After Training : {}\".format(model.embeddings(torch.LongTensor([1])))) # first few random word embeddings again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
